name: Run Rainfall Scraper Every 3 Hours

on:
  schedule:
    - cron: '0 */3 * * *'  # â° Run every 3 hours
  workflow_dispatch:       # ğŸ”˜ Allow manual run from UI

jobs:
  run-script:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v3

      - name: ğŸ Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: ğŸ“¦ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: ğŸ§ª Run scraper script
        run: |
          python scrape_rainfall.py
        continue-on-error: true  # Allow commit step even if scraping failed

      - name: ğŸš€ Commit and push CSV if new data exists
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add *.csv || true
          git diff --cached --quiet || (git commit -m "ğŸ”„ Add latest rainfall data" && git push)
        continue-on-error: true

      - name: ğŸ“¸ Upload screenshot if scraping failed
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: failure-screenshot
          path: table_not_found.png

      - name: ğŸ§¾ Upload debug HTML if available
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: debug-html
          path: debug_page.html
